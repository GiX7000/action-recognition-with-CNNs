{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "n6uwSCuOFMRE",
        "srbPSKNysYaI",
        "kO4to0jH4Afk",
        "yLJwFyMw2zTj",
        "5zh0geg23NGT",
        "mYRlgHEn4WxG"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6SwnvHbMNdP2M/5AL7esN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiX7000/action-recognition-with-CNNs/blob/main/create_src_dir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the parent src directory."
      ],
      "metadata": {
        "id": "n6uwSCuOFMRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"src\", exist_ok=True)"
      ],
      "metadata": {
        "id": "egl8KI5j8INv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### config.py: defines some essential global variables."
      ],
      "metadata": {
        "id": "srbPSKNysYaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/config.py\n",
        "\"\"\" Contains all global variables. \"\"\"\n",
        "import torch.nn as nn\n",
        "\n",
        "batch_size = 32\n",
        "num_channels = 3 # RGB\n",
        "num_classes = 11 # len(label_dict): number of classes for big datasets\n",
        "num_classes_small = 15 # len(small_label_list): number of classes for the small dataset\n",
        "init_hidden_units = [16, 32, 64] # [block1, block2, block3]\n",
        "init_dropout_probs = [0, 0, 0, 0] # [block1, block2, block3, classifier]\n",
        "init_activation_functions = [nn.ReLU(), nn.SELU()] # [feature extractor, classifier]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbxWNd0nsYzq",
        "outputId": "8ca509e3-8278-40d4-a677-023616ddb98d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/config.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils.py: contains functions for data management and processing."
      ],
      "metadata": {
        "id": "kO4to0jH4Afk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/data_utils.py\n",
        "\"\"\" Contains functions for data management. \"\"\"\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "\n",
        "def select_images_by_labels(image_list, label_dict):\n",
        "  \"\"\" Finds and returns the images which belong at the classes of label_dict for the three big datasets. \"\"\"\n",
        "  selected_images = []\n",
        "  for image in image_list:\n",
        "    id = image.split('/')[-1].split('.')[-3][-2:] # extract label from filename\n",
        "    if id.isdigit() and int(id) in label_dict: # this if-statement very important: we retrieve 'str' and want to compare with 'int'\n",
        "      selected_images.append(image)\n",
        "  return selected_images\n",
        "\n",
        "\n",
        "def check_image_type(image_list, type):\n",
        "  \"\"\" Checks if all images in the list are of the given type. \"\"\"\n",
        "  if all(image.endswith(type) for image in image_list):\n",
        "    print(f\"All images in the {image_list} are of type {type}.\")\n",
        "  else:\n",
        "    print(f\"Not all images in the {image_list} are of type {type}!\")\n",
        "\n",
        "\n",
        "def extract_labels(image_list):\n",
        "  \"\"\" Extracts the labels from a list of image paths. \"\"\"\n",
        "  labels_list = []\n",
        "  for image in image_list:\n",
        "    labels = image.split('/')[-1].split('.')[-3][-2:]\n",
        "    labels_list.append(labels)\n",
        "  return labels_list\n",
        "\n",
        "\n",
        "def count_class_occurrences(label_list):\n",
        "  \"\"\" Counts the occurrences of each label in a list. \"\"\"\n",
        "  label_counts = {}\n",
        "  for label in label_list:\n",
        "    if label in label_counts:\n",
        "      label_counts[label] += 1\n",
        "    else:\n",
        "      label_counts[label] = 1\n",
        "  return label_counts\n",
        "\n",
        "\n",
        "def dataset_splits(data_source, split_ratios, is_combined=False):\n",
        "  \"\"\" Splits the data into train, validation, and test sets. \"\"\"\n",
        "\n",
        "  # define the sizes\n",
        "  train_size = int(split_ratios[0] * len(data_source))\n",
        "  val_size = max(2, int(split_ratios[1] * len(data_source)))\n",
        "  #test_size = len(data_source) - train_size - val_size\n",
        "\n",
        "  # shuffle the dataset to ensure the data is randomly distributed\n",
        "  random.shuffle(data_source)\n",
        "\n",
        "  # split the source dataset\n",
        "  train_data = data_source[:train_size]\n",
        "  val_data = data_source[train_size:train_size + val_size]\n",
        "\n",
        "  if is_combined:\n",
        "    return train_data, val_data # no test split if combined datasets\n",
        "  else:\n",
        "    test_data = data_source[train_size + val_size:]\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def dataset_splits_small(data_source, label_list):\n",
        "  \"\"\" Splits the small dataset into train, validation, and test sets. This is done to ensures each class has at least 2 images in each split, if possible. \"\"\"\n",
        "\n",
        "  # group images by class in a dictionary\n",
        "  class_images = {}\n",
        "  for image in data_source:\n",
        "    label = os.path.basename(image).split('.')[0][:-3] # retrieve the label\n",
        "    if label in class_images: # add the image in label key\n",
        "      class_images[label].append(image)\n",
        "    else:\n",
        "      class_images[label] = [image]\n",
        "\n",
        "  # initialize empty lists for train, val and test sets\n",
        "  train_data, val_data, test_data = [], [], []\n",
        "\n",
        "  for label, images in class_images.items():\n",
        "    # shuffle the dataset\n",
        "    random.shuffle(images)\n",
        "\n",
        "    # each class contains 20 shuffled images\n",
        "    train_data.extend(images[:16]) # first 16 for train\n",
        "    val_data.extend(images[16:18]) # second 2 for validation\n",
        "    test_data.extend(images[18:]) # last 2 for test\n",
        "\n",
        "  return train_data, val_data, test_data\n",
        "\n",
        "\n",
        "def copy_images(image_paths, split, is_small_dataset, is_combined_dataset, is_test_set, label_dict, small_label_list, datasets_paths):\n",
        "  \"\"\" Copies any dataset's images to the destination directory. \"\"\"\n",
        "  # datasets_paths = [M_dataset_path, L_dataset_path, R_dataset_path, small_dataset_path,\n",
        "  #                   MR_dataset_path, ML_dataset_path, RL_dataset_path,\n",
        "  #                   test_M_dataset_path, test_L_dataset_path, test_R_dataset_path]\n",
        "\n",
        "  # loop over all image paths\n",
        "  for image_path in image_paths:\n",
        "\n",
        "    # if small_dataset\n",
        "    if is_small_dataset:\n",
        "\n",
        "      # retrieve the label\n",
        "      label = os.path.basename(image_path).split('.')[0][:-3] # to correct\n",
        "\n",
        "      # ensure the label is on the label_dictionary\n",
        "      if not label in small_label_list:\n",
        "        raise ValueError(f\"Invalid label: {label}\")\n",
        "\n",
        "      # set the destination dataset path for the small dataset\n",
        "      dataset_path = datasets_paths[3]\n",
        "\n",
        "      # set the destination directory\n",
        "      dest_dir = os.path.join(dataset_path, split, label)\n",
        "\n",
        "    else: # if individual or combined datasets\n",
        "\n",
        "      # get images's id and class name\n",
        "      img_id = int(image_path.split('/')[-1].split('.')[-3][-2:])\n",
        "      img_name = label_dict[img_id]\n",
        "\n",
        "\n",
        "      # set the destination dataset path for each case\n",
        "\n",
        "      # for MR, ML, RL\n",
        "      if is_combined_dataset:\n",
        "        if 'ResActionsImagesM' in image_path:\n",
        "          # M images to both MR and ML datasets\n",
        "          dest_dir_1 = os.path.join(datasets_paths[4], split, img_name)\n",
        "          os.makedirs(dest_dir_1, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_1) # MR dataset\n",
        "          dest_dir_2 = os.path.join(datasets_paths[5], split, img_name)\n",
        "          os.makedirs(dest_dir_2, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_2) # ML dataset\n",
        "          continue # skip to next image\n",
        "\n",
        "        if 'ResActionsImagesR' in image_path:\n",
        "          # R images to both MR and RL datasets\n",
        "          dest_dir_1 = os.path.join(datasets_paths[4], split, img_name)\n",
        "          os.makedirs(dest_dir_1, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_1) # MR dataset\n",
        "          dest_dir_2 = os.path.join(datasets_paths[6], split, img_name)\n",
        "          os.makedirs(dest_dir_2, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_2) # RL dataset\n",
        "          continue # skip to next image\n",
        "\n",
        "        if 'ResActionsImagesL' in image_path:\n",
        "          # L images to both ML and RL datasets\n",
        "          dest_dir_1 = os.path.join(datasets_paths[5], split, img_name)\n",
        "          os.makedirs(dest_dir_1, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_1) # ML dataset\n",
        "          dest_dir_2 = os.path.join(datasets_paths[6], split, img_name)\n",
        "          os.makedirs(dest_dir_2, exist_ok=True)\n",
        "          shutil.copy(image_path, dest_dir_2) # RL dataset\n",
        "          continue # skip to next image\n",
        "\n",
        "      # M, R, L as test sets\n",
        "      elif is_test_set:\n",
        "        if 'ResActionsImagesM' in image_path:\n",
        "          dataset_path = datasets_paths[7] # M alone as a test set\n",
        "        elif 'ResActionsImagesR' in image_path:\n",
        "          dataset_path = datasets_paths[9] # R alone as a test set\n",
        "        elif 'ResActionsImagesL' in image_path:\n",
        "          dataset_path = datasets_paths[8] # L alone as a test set\n",
        "        else:\n",
        "          raise ValueError(f\"Invalid dataset path: {image_path}.\")\n",
        "\n",
        "      # for M, R, L\n",
        "      else:\n",
        "        if 'ResActionsImagesM' in image_path:\n",
        "          dataset_path = datasets_paths[0] # M dataset\n",
        "        elif 'ResActionsImagesR' in image_path:\n",
        "          dataset_path = datasets_paths[2] # R dataset\n",
        "        elif 'ResActionsImagesL' in image_path:\n",
        "          dataset_path = datasets_paths[1] # L dataset\n",
        "        else:\n",
        "          raise ValueError(f\"Invalid dataset path: {image_path}.\")\n",
        "\n",
        "      # set the destination directory\n",
        "      dest_dir = os.path.join(dataset_path, split, img_name)\n",
        "\n",
        "    # create the destination directory if it doesn't exist for both cases\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # copy the image to the destination directory\n",
        "    shutil.copy(image_path, dest_dir)\n",
        "\n",
        "\n",
        "def image_count(dir):\n",
        "  \"\"\" Counts all image files in the given directory, including subdirectories. \"\"\"\n",
        "\n",
        "  total_count = 0\n",
        "  for root, _, files in os.walk(dir):\n",
        "    count = 0\n",
        "    for file in files:\n",
        "      if file.endswith('.png'):\n",
        "        count += 1\n",
        "    print(f\"For the directory '{root}', counted {count} image(s).\")\n",
        "    total_count += count\n",
        "\n",
        "  print(f\"Total images in '{dir}': {total_count}\")\n",
        "  return total_count\n",
        "\n",
        "\n",
        "def mean_std_calculation(image_paths):\n",
        "  \"\"\" Calculates the mean and standard deviation of a dataset. \"\"\"\n",
        "\n",
        "  # initialize variables\n",
        "  total_mean = np.array([0.0, 0.0, 0.0]) # 3-d for 3 color chhannels\n",
        "  total_std = np.array([0.0, 0.0, 0.0])\n",
        "  n_pixels = 0  # total number of pixels across all images\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    # read the image using plt.imread\n",
        "    image = plt.imread(image_path)  # shape of (H, W, C)\n",
        "\n",
        "    # convert the image to float if needed\n",
        "    #if image.dtype != np.float32:\n",
        "    #    image = image.astype(np.float32) / 255.0  # scale to [0, 1] if raw pixel values\n",
        "\n",
        "    # accumulate mean and std for each channel for the entire dataset\n",
        "    n_pixels += image.shape[0] * image.shape[1]\n",
        "    total_mean += image.mean(axis=(0, 1)) * image.shape[0] * image.shape[1] # weighted contribution of each image to the dataset on the fly!\n",
        "    total_std += ((image - image.mean(axis=(0, 1)))**2).sum(axis=(0, 1))\n",
        "\n",
        "  # mean and std calculations\n",
        "  total_mean /= n_pixels\n",
        "  total_std = np.sqrt(total_std / n_pixels)\n",
        "\n",
        "  print(f\"Dataset Mean (per channel): {total_mean}\")\n",
        "  print(f\"Dataset Std (per channel): {total_std}\")\n",
        "\n",
        "\n",
        "def create_dataloaders(train_dir, val_dir, test_dir, transform, batch_size, num_workers):\n",
        "  \"\"\" Creates training, validation, and test DataLoaders. \"\"\"\n",
        "\n",
        "  # create datasets using ImageFolder\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  val_data = datasets.ImageFolder(val_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  # get class names\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  # check if any classes are empty (came after error!)\n",
        "  if len(train_data.classes) == 0:\n",
        "    print(\"No classes found in the training dataset!\")\n",
        "  if len(val_data.classes) == 0:\n",
        "    print(\"No classes found in the validation dataset!\")\n",
        "  if len(test_data.classes) == 0:\n",
        "    print(\"No classes found in the test dataset!\")\n",
        "\n",
        "  # turn images into dataloaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True # speed up a little\n",
        "  )\n",
        "  val_dataloader = DataLoader(\n",
        "      val_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, class_names\n",
        "\n",
        "\n",
        "def create_combined_dataloaders(train_dir, val_dir, test_dir, train_val_transform, test_transform, batch_size, num_workers):\n",
        "  \"\"\" Creates train/val DataLoaders for combined datasets and test DataLoader for the respective full dataset. \"\"\"\n",
        "\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=train_val_transform)\n",
        "  val_data = datasets.ImageFolder(val_dir, transform=train_val_transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "  # Turn datasets into DataLoaders\n",
        "  train_dataloader = DataLoader( train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True )\n",
        "  val_dataloader = DataLoader( val_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True )\n",
        "  test_dataloader = DataLoader( test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True )\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader\n",
        "\n",
        "print(\"Module data_utils imported successfully!\")"
      ],
      "metadata": {
        "id": "_9YgSknqRkEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5092d6-d94b-49b6-82b1-7e7fdf4ea1da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/data_utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models.py: contains class implementations for various CNN architectures."
      ],
      "metadata": {
        "id": "yLJwFyMw2zTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGjQf0Oj4eO-",
        "outputId": "bc7f3505-962e-4424-9e25-0a280882f9fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/models.py\n",
        "\"\"\" Contains all Pytorch CNN versions. \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from project.src.config import batch_size, num_channels, num_classes, num_classes_small, init_hidden_units, init_dropout_probs, init_activation_functions\n",
        "\n",
        "\n",
        "# baseline CNN model\n",
        "class CNN(nn.Module):\n",
        "  \"\"\" CNN model architecture based on the structure proposed in the given notebook. \"\"\"\n",
        "  def __init__(self, in_features: int, out_features: int, hidden_units: list):\n",
        "    super().__init__() # initialize the initializer!\n",
        "\n",
        "    # A. define the components of nn\n",
        "\n",
        "    # 1. feature extractor blocks\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "\n",
        "    # 2. classifier\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=128),  # we set 1024 as in_features to ensure compatibility\n",
        "        nn.SELU(),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "    )\n",
        "\n",
        "  # B. define how the above components are combined into a CNN (feature extractor + classifier)\n",
        "  def forward(self, x):\n",
        "    x = self.block3(self.block2(self.block1(x)))\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "# CNN model version 2\n",
        "class CNN_v2(nn.Module):\n",
        "  \"\"\" CNN baseline model architecture with dropout layers. \"\"\"\n",
        "  def __init__(self, out_features, in_features=num_channels, hidden_units=init_hidden_units, dropout_probs=init_dropout_probs, activation_functions=init_activation_functions):\n",
        "    super().__init__() # initialize the initializer!\n",
        "\n",
        "    # A. define the components of nn\n",
        "\n",
        "    # 1. feature extractor blocks\n",
        "    self.block1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]) # spatial dropout for conv2d\n",
        "    )\n",
        "\n",
        "    self.block2 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]) # spatial dropout for conv2d\n",
        "    )\n",
        "\n",
        "    self.block3 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2]) # spatial dropout for conv2d\n",
        "    )\n",
        "\n",
        "    # dynamically calculate the output of the feaature extractor/classifier's input by passing a dummy input\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = self.block3(self.block2(self.block1(dummy_input))) # shape of (batch_size, channels, height, width)\n",
        "      #print(output.shape)\n",
        "      classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # 2. classifier\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),  # we set in_features dynamically through a dummy pass to ensure compatibility\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]), # standarrd dropout for fully connected layers\n",
        "        nn.Linear(in_features=128, out_features=out_features) # dynamically setting the number of classes\n",
        "    )\n",
        "\n",
        "  # B. define how the above components are combined into a CNN (feature extractor + classifier)\n",
        "  def forward(self, x):\n",
        "    x = self.block3(self.block2(self.block1(x)))\n",
        "    x = self.classifier(x)\n",
        "    return x\n",
        "\n",
        "print(\"Module models imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### engine.py: contains functions for training and testing models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5zh0geg23NGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/engine.py\n",
        "\"\"\" Contains functions for training and testing a Pytorch model. \"\"\"\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# train step function\n",
        "def train_step(model, dataloader, loss_fn, optimizer, device, regularization=None, reg_lambda=0.0, max_norm=2.0):\n",
        "  \"\"\" Trains a PyTorch model for a single epoch. \"\"\"\n",
        "\n",
        "  # put the model in a train mode\n",
        "  model.train()\n",
        "\n",
        "  # setup train loss and accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # loop through dataloader and data batches\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # ensure that data is on target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # 1. forward pass\n",
        "    y_pred = model(X)\n",
        "\n",
        "    # 2. calculate loss\n",
        "    loss = loss_fn(y_pred, y)\n",
        "\n",
        "    # add L1 or L2 regularization, if specified\n",
        "    if regularization == 'L1':\n",
        "      l1_reg = 0\n",
        "      for param in model.parameters():\n",
        "        if param.requires_grad: # ensure to apply L2 on trainable params\n",
        "          l1_reg += torch.norm(param, p=1)\n",
        "      loss += reg_lambda * l1_reg\n",
        "    elif regularization == 'L2':\n",
        "      l2_reg = 0\n",
        "      for param in model.parameters():\n",
        "        if param.requires_grad: # ensure to apply L2 on trainable params\n",
        "          l2_reg += torch.norm(param, p=2)\n",
        "      loss += reg_lambda * l2_reg\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # 3. optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # apply weight constraints after each optimizer step\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'weight' in name: # apply constraints to only weights\n",
        "        param.data.clamp_(-max_norm, max_norm) # in-place operation\n",
        "\n",
        "    # 6. calculate and accumulate accuracy\n",
        "    #y_pred_probs = torch.softmax(y_pred, dim=1) # apply softmax to get probabilities\n",
        "    y_pred_class = torch.argmax(y_pred, dim=1) # get the max of raw logits\n",
        "    train_acc += ((y_pred_class == y).sum().item()/len(y_pred_class))\n",
        "\n",
        "  # adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "# test set function\n",
        "def test_step(model, dataloader, loss_fn, device, regularization=None, reg_lambda=0.0):\n",
        "  \"\"\" Tests a PyTorch model for a single epoch. \"\"\"\n",
        "\n",
        "  # put the model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # setup test loss and accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # storage lists for actual and predicted labels\n",
        "  all_y_true = []\n",
        "  all_y_preds = []\n",
        "\n",
        "  # turn on inference context manager\n",
        "  with torch.inference_mode():\n",
        "    # loop through dataloader and data batches\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # ensure that the data is on the target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. forward pass\n",
        "      test_pred_logits = model(X)\n",
        "\n",
        "      # 2. calculate loss\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "\n",
        "      # add L1 or L2 regularization, if specified\n",
        "      if regularization == 'L1':\n",
        "        l1_reg = 0\n",
        "        for param in model.parameters():\n",
        "          if param.requires_grad: # ensure to apply L2 on trainable params\n",
        "            l1_reg += torch.norm(param, p=1)\n",
        "        loss += reg_lambda * l1_reg\n",
        "      elif regularization == 'L2':\n",
        "        l2_reg = 0\n",
        "        for param in model.parameters():\n",
        "          if param.requires_grad: # ensure to apply L2 on trainable params\n",
        "            l2_reg += torch.norm(param, p=2)\n",
        "        loss += reg_lambda * l2_reg\n",
        "\n",
        "      test_loss += loss.item()\n",
        "\n",
        "      # 3. calculate and accumulate accuracy across all batches\n",
        "      #test_pred_probs = torch.softmax(test_pred_logits, dim=1)\n",
        "      test_pred_labels = torch.argmax(test_pred_logits, dim=1) # index of best prediction\n",
        "      test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "      # 4. store actual and predicted labels to lists\n",
        "      all_y_true.extend(y.cpu().numpy())\n",
        "      all_y_preds.extend(test_pred_labels.cpu().numpy())\n",
        "\n",
        "  # adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc, all_y_true, all_y_preds\n",
        "\n",
        "# train function: train step for epochs\n",
        "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, regularization=None, reg_lambda=0.0, early_stopping=False, patience=5, delta=1e-5):\n",
        "  \"\"\" Trains and tests a PyTorch model with optional regulariation and early stopping. \"\"\"\n",
        "\n",
        "  # create a results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "             \"train_acc\": [],\n",
        "             \"val_loss\": [],\n",
        "             \"val_acc\": [],\n",
        "             \"best_epoch\": 0,\n",
        "             \"stopped_epoch\":-1}\n",
        "\n",
        "  # early stopping variables\n",
        "  best_acc = 0\n",
        "  epochs_without_improvement = 0\n",
        "  best_model_weights = None\n",
        "\n",
        "  # loop through training and test steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_step(model=model,\n",
        "                                       dataloader=train_dataloader,\n",
        "                                       loss_fn=loss_fn,\n",
        "                                       optimizer=optimizer,\n",
        "                                       device=device,\n",
        "                                       regularization=regularization,\n",
        "                                       reg_lambda=reg_lambda)\n",
        "    test_loss, test_acc, y_true, y_preds = test_step(model=model,\n",
        "                                                     dataloader=test_dataloader,\n",
        "                                                     loss_fn=loss_fn,\n",
        "                                                     device=device,\n",
        "                                                     regularization=regularization,\n",
        "                                                     reg_lambda=reg_lambda)\n",
        "\n",
        "    # print what's happening\n",
        "    print(\n",
        "      f\"Epoch: {epoch+1} | \"\n",
        "      f\"train_loss: {train_loss:.4f} | \"\n",
        "      f\"train_acc: {train_acc:.4f} | \"\n",
        "      f\"val_loss: {test_loss:.4f} | \"\n",
        "      f\"val_acc: {test_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # update results dictionary\n",
        "    results[\"train_loss\"].append(train_loss)\n",
        "    results[\"train_acc\"].append(train_acc)\n",
        "    results[\"val_loss\"].append(test_loss)\n",
        "    results[\"val_acc\"].append(test_acc)\n",
        "\n",
        "    # also save actual and predicted labels\n",
        "    results[\"y_true\"] = y_true\n",
        "    results[\"y_preds\"] = y_preds\n",
        "\n",
        "    # early stopping process\n",
        "    if early_stopping:\n",
        "      if test_acc > best_acc + delta: # or it could be loss: (test_loss < best_loss - delta)\n",
        "        best_acc = test_acc\n",
        "        best_model_weights = model.state_dict() # if no improvement this variable remains equal to None!\n",
        "        epochs_without_improvement = 0\n",
        "        results[\"best_epoch\"] = epoch + 1 # (epoch is zero based)\n",
        "      else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "          print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
        "          results[\"stopped_epoch\"] = epoch + 1\n",
        "          break\n",
        "\n",
        "  # restore and save best weights, if early stopping is requested\n",
        "  if early_stopping:\n",
        "    if best_model_weights is not None:\n",
        "      model.load_state_dict(best_model_weights)\n",
        "      print(f\"Model restored from epoch {results['best_epoch']}.\")\n",
        "    else:\n",
        "      print(\"No improvement during training. Use of final model's weights.\")\n",
        "\n",
        "  # return the filled results at the end of the epochs\n",
        "  return results\n",
        "\n",
        "\n",
        "def eval_model(model, dataloader, loss_fn, device):\n",
        "  \"\"\" Makes predictions o the test set and returns a dictionary with the results. \"\"\"\n",
        "\n",
        "  # initialize test loss and accuracy\n",
        "  loss, acc = 0, 0\n",
        "\n",
        "  # storage lists for actual and predicted labels\n",
        "  actual_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  # set the evaluation mode\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in tqdm(dataloader):\n",
        "\n",
        "      # ensure that they are on device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # make prediction with the model\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # calculate loss and accuracy\n",
        "      loss += loss_fn(y_pred, y)\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1) # to predict: logits->probs->labels\n",
        "      acc += (y_pred_class == y).sum().item()/len(y_pred_class)\n",
        "\n",
        "      # store predictions and actual labels\n",
        "      actual_labels.extend(y.cpu().numpy())\n",
        "      predicted_labels.extend(y_pred_class.cpu().numpy())\n",
        "\n",
        "    # adjust metrics to get average loss and accuracy per batch\n",
        "    loss = loss / len(dataloader)\n",
        "    acc = acc / len(dataloader)\n",
        "\n",
        "  return {\"model_name\": model.__class__.__name__, # only works when model was created as a class\n",
        "          \"test_loss\": loss.item(),\n",
        "          \"test_acc\": acc,\n",
        "          \"y_true\": actual_labels,\n",
        "          \"y_preds\": predicted_labels}\n",
        "\n",
        "print(\"Module engine imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCHg3L4C8a1Y",
        "outputId": "caac3180-fd6a-4dbc-b756-304ce8b10de2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### experiments.py: includes all functions for conducting experiments."
      ],
      "metadata": {
        "id": "DS_dSb0P3s1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/experiments.py\n",
        "\"\"\" Contains functions for running experiments. \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from project.src.engine import train\n",
        "from project.src.models import CNN, CNN_v2\n",
        "from project.src.data_utils import create_dataloaders, create_combined_dataloaders\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import itertools\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# experiment 1\n",
        "def experiment_l1_regularization(train_dataloader, val_dataloader, num_epochs, device, lambda_values_list, num_channels, num_classes, init_hidden_units):\n",
        "  \"\"\" Experiment with L1 Regularization with different lambda regularization values. \"\"\"\n",
        "  print(\"Experimenting with L1 Regularization\")\n",
        "\n",
        "  # lambda regularization values to experiment with\n",
        "  lambda_values = lambda_values_list # [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]  # np.logspace(-2, -6, num=5)\n",
        "  print(\"Lambda regularization values to try:\", lambda_values)\n",
        "\n",
        "  # storage dictionary for all results\n",
        "  model_all_results = {}\n",
        "\n",
        "  # loop over all lambda_values\n",
        "  start_time = time.time()\n",
        "  for decay in lambda_values:\n",
        "    print(f\"\\nTesting with lambda: {decay}\")\n",
        "\n",
        "    # create a new model's instance for the current lambda\n",
        "    model = CNN(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units).to(device)\n",
        "\n",
        "    # setup loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001)\n",
        "\n",
        "    # start training\n",
        "    model_results = train(model=model,\n",
        "                          train_dataloader=train_dataloader,\n",
        "                          test_dataloader=val_dataloader,\n",
        "                          optimizer=optimizer,\n",
        "                          loss_fn=loss_fn,\n",
        "                          epochs=num_epochs,\n",
        "                          device=device,\n",
        "                          regularization='L1',\n",
        "                          reg_lambda=decay)\n",
        "\n",
        "    # save the results for the current weight decay\n",
        "    model_all_results[decay] = model_results\n",
        "\n",
        "  # calculate and print the total time for this experiment\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time for the model with L1 regularization: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate the total trainable parameters\n",
        "  model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable parameters: {model_total_params}\")\n",
        "\n",
        "  return model_all_results\n",
        "\n",
        "\n",
        "# experiment 2\n",
        "def experiment_l2_regularization(train_dataloader, val_dataloader, num_epochs, device, weight_decay_values_list, num_channels, num_classes, init_hidden_units):\n",
        "  \"\"\" Experiment with L2 Regularization with different weight decay values. \"\"\"\n",
        "  print(\"Experimenting with L2 Regularization\")\n",
        "\n",
        "  # weight decay values to experiment with\n",
        "  weight_decay_values = weight_decay_values_list # [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "  print(\"Weight decay values to try:\", weight_decay_values)\n",
        "\n",
        "  # storage dictionary for all results\n",
        "  model_all_results = {}\n",
        "\n",
        "  # loop over all weight decay values\n",
        "  start_time = time.time()\n",
        "  for decay in weight_decay_values:\n",
        "    print(f\"\\nTesting with weight decay: {decay}\")\n",
        "\n",
        "    # create model instance\n",
        "    model = CNN(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units).to(device)\n",
        "\n",
        "    # Setup loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=decay)\n",
        "\n",
        "    # start training\n",
        "    model_results = train(model=model,\n",
        "                             train_dataloader=train_dataloader,\n",
        "                             test_dataloader=val_dataloader,\n",
        "                             optimizer=optimizer,\n",
        "                             loss_fn=loss_fn,\n",
        "                             epochs=num_epochs,\n",
        "                             device=device)\n",
        "\n",
        "    # save the results for the current weight decay\n",
        "    model_all_results[decay] = model_results\n",
        "\n",
        "  # calculate and print the total time for this experiment\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time for the model_1 with L2 regularization: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate the total trainable parameters\n",
        "  model_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable parameters: {model_total_params}\")\n",
        "\n",
        "  return model_all_results\n",
        "\n",
        "\n",
        "# experiment 3\n",
        "def experiment_dropout_regularization(train_dataloader, val_dataloader, num_epochs, device, num_classes, best_decay_value, dropout_probs_combos_list):\n",
        "  \"\"\" Experiment with Dropout Regularization Layers. \"\"\"\n",
        "  print(\"Experimenting with Dropout Regularization Layers\")\n",
        "\n",
        "  # set dropout probabilities\n",
        "  dropout_probs = [0.1, 0.3, 0.5, 0.6, 0.7, 0.9]\n",
        "  print(\"Dropout probabilities to try:\", dropout_probs)\n",
        "\n",
        "  # all possible combinations\n",
        "  all_dropout_probs_combinations = list(itertools.product(dropout_probs, repeat=4))\n",
        "  print(\"Total number of combinations to ideally try:\", len(all_dropout_probs_combinations))\n",
        "\n",
        "  # set predefined good combinations\n",
        "  all_good_dropout_probs_combinations = dropout_probs_combos_list\n",
        "  #  [  [0.5, 0.5, 0.5, 0.5], [0.5, 0.5, 0.4, 0.5], [0.5, 0.4, 0.5, 0.5],\n",
        "  #     [0.5, 0.4, 0.4, 0.5], [0.5, 0.5, 0.5, 0.4], [0.4, 0.5, 0.5, 0.5],\n",
        "  #     [0.4, 0.5, 0.5, 0.4], [0.5, 0.5, 0.5, 0.7], [0.8, 0.5, 0.5, 0.5],\n",
        "  #     [0.8, 0.7, 0.7, 0.6], [0.8, 0.8, 0.7, 0.5], [0.8, 0.5, 0.7, 0.5],\n",
        "  #     [0.6, 0.5, 0.5, 0.5], [0.5, 0.6, 0.6, 0.5], [0.7, 0.6, 0.5, 0.5] ]\n",
        "  print(\"\\nSome possible 'good' combinations to try:\", all_good_dropout_probs_combinations)\n",
        "\n",
        "  # storage dictionary for results\n",
        "  all_results = {}\n",
        "\n",
        "  # loop over all good dropout combinations\n",
        "  start_time = time.time()\n",
        "  for combo in all_good_dropout_probs_combinations:\n",
        "    conv_dropout_prob = combo[:3]\n",
        "    classifier_dropout_prob = combo[3]\n",
        "    print(f\"\\nTesting with conv_dropout_prob: {conv_dropout_prob} and classifier dropout probability: {classifier_dropout_prob}\")\n",
        "\n",
        "    # create model instance\n",
        "    model = CNN_v2(out_features=num_classes, dropout_probs=combo).to(device)\n",
        "\n",
        "    # setup loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "    # start training\n",
        "    results = train(model=model,\n",
        "                    train_dataloader=train_dataloader,\n",
        "                    test_dataloader=val_dataloader,\n",
        "                    optimizer=optimizer,\n",
        "                    loss_fn=loss_fn,\n",
        "                    epochs=num_epochs,\n",
        "                    device=device)\n",
        "\n",
        "    # save results\n",
        "    all_results[tuple(combo)] = results\n",
        "\n",
        "  # calculate and print total experiment time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return all_results\n",
        "\n",
        "\n",
        "# experiment 4\n",
        "def experiment_avgpool_instead_of_maxpool(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with applying AvgPool2d instead of MaxPool2d. \"\"\"\n",
        "  print(\"Experimenting with AveragePooling instead of MaxPooling layers\")\n",
        "\n",
        "  # model function with AveragePooling layers\n",
        "  def cnn_with_avg_pooling(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with AveragePooling instead of MaxPooling layers. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_with_avg_pooling().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 5\n",
        "def experiment_batch_norm(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with adding BatchNorm layers. \"\"\"\n",
        "  print(\"Experimenting with BatchNorm layers\")\n",
        "\n",
        "  # model function with BatchNorm layers\n",
        "  def cnn_with_batch_norm(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with BatchNorm layers. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        nn.BatchNorm2d(hidden_units[0]),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        nn.BatchNorm2d(hidden_units[1]),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        nn.BatchNorm2d(hidden_units[1]),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        nn.BatchNorm2d(hidden_units[2]),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_with_batch_norm().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 6\n",
        "def experiment_extra_block(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, new_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with adding an extra block in feature extractor. \"\"\"\n",
        "  print(\"Experimenting with extra block in feature extractor\")\n",
        "\n",
        "  # model function with an extra block\n",
        "  def cnn_with_extra_block(in_features=num_channels, out_features=num_classes, hidden_units=new_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with an extra block in feature extractor. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2]),\n",
        "\n",
        "        # extra block\n",
        "        nn.Conv2d(in_channels=hidden_units[2], out_channels=hidden_units[3], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=1, stride=1),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_with_extra_block().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 7\n",
        "def experiment_extra_layers_classifier(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with adding extra layers in classifier. \"\"\"\n",
        "  print(\"Experimenting with extra layers in classifier\")\n",
        "\n",
        "  # model function with extra layers in classifier\n",
        "  def cnn_with_enhanced_classifier(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with extra layer in classifier. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=out_features, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_with_enhanced_classifier().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 8\n",
        "def experiment_enhanced_blocks(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions,  best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with enhanced blocks like block 2. \"\"\"\n",
        "  print(\"Experimenting with enhanced blocks like block 2\")\n",
        "\n",
        "  # model function with enhanced blocks\n",
        "  def cnn_enhanced_blocks(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with all blocks enhanced. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[0], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[2], out_channels=hidden_units[2], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_enhanced_blocks().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 9\n",
        "def experiment_simplified_block2(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with simplified block2. \"\"\"\n",
        "  print(\"Experimenting with simplified block2\")\n",
        "\n",
        "  # model function with simplified block2\n",
        "  def cnn_simplified_block2(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with simplified block2. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128),\n",
        "        activation_functions[1],\n",
        "        nn.Dropout(p=dropout_probs[3]),\n",
        "        nn.Linear(in_features=128, out_features=out_features)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_simplified_block2().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 10\n",
        "def experiment_simplified_block_and_classifier(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with simplified block2 and classifier. \"\"\"\n",
        "  print(\"Experimenting with simplified block2 and classifier\")\n",
        "\n",
        "  # model function with simplified block2 and classifier\n",
        "  def cnn_simplified_block_and_classifier(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with simplified block2 and classifier. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_simplified_block_and_classifier().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 11\n",
        "def experiment_simplified_block_and_classifier_enhanced(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with simplified block 2 and classifier plus out_features=256. \"\"\"\n",
        "  print(\"Experimenting with simplified block 2 and classifier plus out_features=256\")\n",
        "\n",
        "  # model function with simplified block 2 and classifier\n",
        "  def cnn_simplified_block_and_classifier_enhanced(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with simplified block 2 and classifier (out_features=256). \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[1], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[1], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[1]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[1], out_channels=hidden_units[2], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=256)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_simplified_block_and_classifier_enhanced().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 12\n",
        "def experiment_simplified_block_and_classifier_without_block3(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo):\n",
        "  \"\"\" Experiment with simplified block2 and classifier and dropped block 3. \"\"\"\n",
        "  print(\"Experimenting with simplified block2 and classifier and dropped block 3\")\n",
        "\n",
        "  # Model function with simplified block2 and classifier without block 3\n",
        "  def cnn_simplified_block_and_classifier_without_block3(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo):\n",
        "    \"\"\" CNN model with simplified block2 and classifier and dropped block 3. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=3, padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[2], kernel_size=3, padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_simplified_block_and_classifier_without_block3().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 13\n",
        "def experiment_kernel_sizes(train_dataloader, val_dataloader, num_epochs, device, num_channels, num_classes, init_hidden_units, init_activation_functions, best_decay_value, best_dropout_probs_combo, kernel_sizes_list):\n",
        "  \"\"\" Experiment with modifying kernel sizes. \"\"\"\n",
        "  print(\"Experimenting with modifying kernel sizes\")\n",
        "\n",
        "  # model function with different kernel sizes\n",
        "  def cnn_kernel_sizes(in_features=num_channels, out_features=num_classes, hidden_units=init_hidden_units, activation_functions=init_activation_functions, dropout_probs=best_dropout_probs_combo, kernel_sizes=kernel_sizes_list):\n",
        "    \"\"\" CNN model with different kernel sizes. \"\"\"\n",
        "    feature_extractor = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_features, out_channels=hidden_units[0], kernel_size=kernel_sizes[0], padding='valid'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[0]),\n",
        "\n",
        "        nn.Conv2d(in_channels=hidden_units[0], out_channels=hidden_units[2], kernel_size=kernel_sizes[1], padding='same'),\n",
        "        activation_functions[0],\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        nn.Dropout2d(p=dropout_probs[2])\n",
        "        )\n",
        "\n",
        "    # calculate feature extractor's output shape\n",
        "    dummy_input = torch.randn(1, in_features, 25, 149)\n",
        "    with torch.no_grad():\n",
        "      output = feature_extractor(dummy_input)\n",
        "    classifier_input_size = output.shape[1] * output.shape[2] * output.shape[3]\n",
        "\n",
        "    # classifier\n",
        "    classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=classifier_input_size, out_features=128)\n",
        "        )\n",
        "\n",
        "    return nn.Sequential(feature_extractor, classifier)\n",
        "\n",
        "  # create model instance\n",
        "  model = cnn_kernel_sizes().to(device)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.RMSprop(params=model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device)\n",
        "\n",
        "  # calculate and print total training time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 14\n",
        "def experiment_hidden_layer_sizes(train_dataloader, val_dataloader, num_epochs, device, num_classes, best_decay_value, best_dropout_probs_combo, hidden_layer_sizes_list):\n",
        "  \"\"\" Experiment with different hidden layer sizes. \"\"\"\n",
        "  print(\"Experimenting with Hidden Layer Sizes\")\n",
        "\n",
        "  # generate some good combinations\n",
        "  hidden_layer_sizes_combinations = hidden_layer_sizes_list # [(32, 64, 64), (32, 32, 128), (64, 32, 64), (32, 32, 64), (64, 64, 32), (32, 64, 32)]\n",
        "  print(\"Hidden layer sizes combinations to try:\", hidden_layer_sizes_combinations)\n",
        "\n",
        "  # storage dictionary for results\n",
        "  all_results = {}\n",
        "\n",
        "  # loop over all combinations\n",
        "  start_time = time.time()\n",
        "  for combo in hidden_layer_sizes_combinations:\n",
        "    print(f\"\\nTesting hidden layer sizes combination: {combo}...\")\n",
        "\n",
        "    # initialize model for the current combination\n",
        "    model = CNN_v2(out_features=num_classes, hidden_units=combo, dropout_probs=best_dropout_probs_combo).to(device)\n",
        "\n",
        "    # set loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=best_decay_value)\n",
        "\n",
        "    # start training\n",
        "    start_time_2 = time.time()\n",
        "    results = train(model=model,\n",
        "                    train_dataloader=train_dataloader,\n",
        "                    test_dataloader=val_dataloader,\n",
        "                    optimizer=optimizer,\n",
        "                    loss_fn=loss_fn,\n",
        "                    epochs=num_epochs,\n",
        "                    device=device)\n",
        "    end_time_2 = time.time()\n",
        "\n",
        "    # compute generalization gap\n",
        "    generalization_gap = results['train_acc'][-1] - results['val_acc'][-1]\n",
        "\n",
        "    # save the results in the dictionary\n",
        "    results['generalization_gap'] = generalization_gap\n",
        "    all_results[combo] = results\n",
        "\n",
        "    # print best results for the current combination\n",
        "    print(f\"\\nHidden layer sizes combination: {combo}\")\n",
        "    print(f\"Validation accuracy: {results['val_acc'][-1]:.2f}\")\n",
        "    print(f\"Generalization gap: {generalization_gap:.2f}\") # Want this negative (not overfitting)\n",
        "    print(f\"Total training time: {end_time_2 - start_time_2:.3f} seconds\")\n",
        "\n",
        "  # calculate and print total experiment time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return all_results\n",
        "\n",
        "\n",
        "# experiment 15\n",
        "def experiment_optimizers_and_learning_rates(train_dataloader, val_dataloader, num_epochs, device, num_classes, best_hidden_units_combo, best_decay_value, best_dropout_probs_combo, learning_rates_list):\n",
        "  \"\"\" Experiment with optimizers and learning rates. \"\"\"\n",
        "  print(\"Experimenting with Optimizers and Learning Rates\")\n",
        "\n",
        "  # define learning rates to try\n",
        "  learning_rates = learning_rates_list # [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
        "  print(\"Learning rates to try:\", learning_rates)\n",
        "\n",
        "  # define optimizers to try\n",
        "  optimizers = {\n",
        "      'SGD': torch.optim.SGD,\n",
        "      #'SGD + momentum': lambda params, lr: torch.optim.SGD(params, lr=lr, momentum=0.9, nesterov=True, weight_decay=best_decay_value),\n",
        "      #'Adagrad': torch.optim.Adagrad,\n",
        "      'RMSprop': torch.optim.RMSprop,\n",
        "      'Adam': torch.optim.Adam,\n",
        "      'AdamW': torch.optim.AdamW,\n",
        "      'NAdam': torch.optim.NAdam\n",
        "    }\n",
        "  print(\"Optimizers to try:\", list(optimizers.keys()))\n",
        "\n",
        "  # store learning rate for best validation accuracy for each optimizer\n",
        "  optim_results_list = []\n",
        "\n",
        "  # loop over optimizers\n",
        "  start_time = time.time()\n",
        "  for optimizer_name, optimizer_fn in optimizers.items():\n",
        "    print(f\"\\nTesting {optimizer_name} optimizer...\")\n",
        "\n",
        "    # track best learning rate and accuracy for the current optimizer\n",
        "    best_lr = None\n",
        "    best_optim_val_accuracy = 0\n",
        "\n",
        "    # loop over learning rates\n",
        "    for lr in learning_rates:\n",
        "      print(f\"\\nLearning rate: {lr}... \\n\")\n",
        "\n",
        "      # initialize model for each optimizer and learning rate\n",
        "      model = CNN_v2(out_features=num_classes, hidden_units=best_hidden_units_combo, dropout_probs=best_dropout_probs_combo).to(device)\n",
        "\n",
        "      # set loss function and optimizer\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      if optimizer_name == 'SGD + momentum':\n",
        "        optimizer = optimizer_fn(model.parameters(), lr=lr)\n",
        "      else:\n",
        "        optimizer = optimizer_fn(model.parameters(), lr=lr, weight_decay=best_decay_value)\n",
        "\n",
        "      # start training\n",
        "      results = train(model=model,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      test_dataloader=val_dataloader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=num_epochs,\n",
        "                      device=device)\n",
        "\n",
        "      # retrieve validation accuracy for the current combination\n",
        "      val_accuracy = results['val_acc'][-1]\n",
        "\n",
        "      # keep track of best validation accuracy\n",
        "      if val_accuracy > best_optim_val_accuracy:\n",
        "        best_optim_val_accuracy = val_accuracy\n",
        "        best_lr = lr\n",
        "\n",
        "    # save the results in a list\n",
        "    optim_results_list.append((optimizer_name, best_lr, best_optim_val_accuracy))\n",
        "\n",
        "    # print best results for the current optimizer\n",
        "    print(f\"\\nBest learning rate for {optimizer_name}: {best_lr}\")\n",
        "    print(f\"Best validation accuracy for {optimizer_name}: {best_optim_val_accuracy:.2f}%\")\n",
        "\n",
        "  # calculate and print total experiment time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return optim_results_list\n",
        "\n",
        "\n",
        "# experiment 16\n",
        "def experiment_activation_functions(train_dataloader, val_dataloader, num_epochs, device, num_classes, best_hidden_units_combo, best_decay_value, best_dropout_probs_combo, best_optimizer, best_learning_rate):\n",
        "  \"\"\" Experiment with activation functions. \"\"\"\n",
        "  print(\"Experimenting with Activation Functions\")\n",
        "\n",
        "  # define activation functions to try\n",
        "  activation_functions = [\n",
        "      ('ReLU', nn.ReLU()),\n",
        "      ('LeakyReLU', nn.LeakyReLU(negative_slope=0.01)),\n",
        "      ('ELU', nn.ELU()),\n",
        "      ('GELU', nn.GELU()),\n",
        "      #('PReLU'), nn.PReLU(),\n",
        "      ('SiLU', nn.SiLU()),\n",
        "      #('Mish', nn.Mish())\n",
        "    ]\n",
        "  print(\"Activation functions to try:\", [name for name, _ in activation_functions])\n",
        "\n",
        "  # store results\n",
        "  activation_results_list = []\n",
        "\n",
        "  # loop over activation functions\n",
        "  start_time = time.time()\n",
        "  for activation_name1, activation_fn1 in activation_functions:\n",
        "    print(f\"\\nTesting {activation_name1} activation function as activation function in feature extractor...\")\n",
        "\n",
        "    for activation_name2, activation_fn2 in activation_functions:\n",
        "      print(f\"\\nTesting {activation_name2} activation function as activation function in classifier...\")\n",
        "\n",
        "      # initialize model\n",
        "      model = CNN_v2(out_features=num_classes, hidden_units=best_hidden_units_combo, dropout_probs=best_dropout_probs_combo, activation_functions=[activation_fn1, activation_fn2]).to(device)\n",
        "\n",
        "      # set loss function and optimizer\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      optimizer = best_optimizer(model.parameters(), lr=best_learning_rate, weight_decay=best_decay_value)\n",
        "\n",
        "      # start training\n",
        "      results = train(model=model,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      test_dataloader=val_dataloader,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=num_epochs,\n",
        "                      device=device)\n",
        "\n",
        "      # retrieve validation accuracy\n",
        "      val_accuracy = results['val_acc'][-1]\n",
        "\n",
        "      # save results\n",
        "      activation_results_list.append((activation_name1, activation_name2, val_accuracy))\n",
        "\n",
        "      # print best results for the current activation functions combination\n",
        "      print(f\"\\nActivation function for feature extractor: {activation_name1}\")\n",
        "      print(f\"Activation function for classifier: {activation_name2}\")\n",
        "      print(f\"Validation accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "  # calculate and print total experiment time\n",
        "  end_time = time.time()\n",
        "  print(f\"\\nTotal experiment's time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "  return activation_results_list\n",
        "\n",
        "\n",
        "# experiment 17\n",
        "def experiment_batch_sizes(train_data, val_data, test_data, num_epochs, device, num_classes, best_hidden_units_combo, best_activation_functions, best_decay_value, best_dropout_probs_combo, best_optimizer, best_learning_rate, batch_sizes_list, num_workers, train_val_transform, is_combined=False, test_transform=None):\n",
        "  \"\"\" Experiment with batch sizes. \"\"\"\n",
        "  print(\"Experimenting with Batch Sizes\")\n",
        "\n",
        "  # define batch sizes to try\n",
        "  batch_sizes = batch_sizes_list # [16, 32, 64, 128]\n",
        "  print(\"Batch sizes to try:\", batch_sizes)\n",
        "\n",
        "  # store results\n",
        "  batch_size_results_list = []\n",
        "\n",
        "  # loop over batch sizes\n",
        "  for batch_size in batch_sizes:\n",
        "    print(f\"\\nTesting batch size: {batch_size}...\")\n",
        "\n",
        "    # create new dataloaders\n",
        "    if is_combined:\n",
        "      train_dataloader, val_dataloader, test_dataloader, class_names = create_dataloaders_combined(train_data, val_data, test_data, train_val_transform, test_transform, batch_size, num_workers)\n",
        "    else:\n",
        "      train_dataloader, val_dataloader, test_dataloader, class_names = create_dataloaders(train_data, val_data, test_data, train_val_transform, batch_size, num_workers)\n",
        "\n",
        "    # initialize model for the current batch size\n",
        "    model = CNN_v2(out_features=num_classes, hidden_units=best_hidden_units_combo, dropout_probs=best_dropout_probs_combo, activation_functions=best_activation_functions).to(device)\n",
        "\n",
        "    # set loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = best_optimizer(params=model.parameters(), lr=best_learning_rate, weight_decay=best_decay_value)\n",
        "\n",
        "    # start training\n",
        "    start_time = time.time()\n",
        "    results = train(model=model,\n",
        "                    train_dataloader=train_dataloader,\n",
        "                    test_dataloader=val_dataloader,\n",
        "                    optimizer=optimizer,\n",
        "                    loss_fn=loss_fn,\n",
        "                    epochs=num_epochs,\n",
        "                    device=device)\n",
        "\n",
        "    # calculate and print total training time\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTotal training time: {end_time - start_time:.3f} seconds\")\n",
        "\n",
        "    # calculate total trainable parameters\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total trainable params: {trainable_params}\")\n",
        "\n",
        "    # save results\n",
        "    batch_size_results_list.append((batch_size, results['val_acc'][-1]))\n",
        "\n",
        "    # print best results for the current batch size\n",
        "    print(f\"\\nBatch size: {batch_size}\")\n",
        "    print(f\"Validation accuracy: {results['val_acc'][-1]:.4f}\\n\")\n",
        "\n",
        "  return batch_size_results_list\n",
        "\n",
        "\n",
        "# experiment 18\n",
        "def cross_validation(dataset, num_epochs, device, num_classes, num_folds, best_config, num_workers, load_weights=False, best_weights=None):\n",
        "  \"\"\" Applies cross-validation to a given dataset and model. \"\"\"\n",
        "  print(\"Cross-Validation training ...\")\n",
        "\n",
        "  # unpack the dictionary\n",
        "  best_batch_size = best_config['best_batch_size']\n",
        "  best_hidden_units_combo = best_config['best_hidden_units_combo']\n",
        "  best_dropout_probs_combo = best_config['best_dropout_probs_combo']\n",
        "  best_optimizer = best_config['best_optimizer']\n",
        "  best_learning_rate = best_config['best_learning_rate']\n",
        "  best_activation_functions = best_config['best_activation_functions']\n",
        "  best_decay_value = best_config['best_decay_value']\n",
        "  loss_fn = best_config['loss_fn']\n",
        "  epochs = best_config['epochs']\n",
        "\n",
        "  # storage dictionary for results\n",
        "  cross_valid_results = {}\n",
        "\n",
        "  # shuffle dataset indices\n",
        "  indices = list(range(len(dataset)))\n",
        "  random.shuffle(indices)\n",
        "\n",
        "  # define fold size\n",
        "  fold_size = len(dataset) // num_folds\n",
        "  remainders = len(dataset) % num_folds\n",
        "  print(f\"Dataset size: {len(dataset)}\")\n",
        "  print(f\"Number of folds: {num_folds}\")\n",
        "  print(f\"Fold size: {fold_size}\")\n",
        "\n",
        "  # loop over the folds\n",
        "  for fold in range(num_folds):\n",
        "    print(f\"\\nFold {fold + 1}/{num_folds}\")\n",
        "\n",
        "    # define validation indices and training indices\n",
        "    start_idx, end_idx = fold * fold_size, (fold + 1) * fold_size + remainders\n",
        "    val_indices = indices[start_idx:end_idx]\n",
        "    train_indices = [i for i in indices if i not in val_indices]\n",
        "\n",
        "    # create dataset subsets\n",
        "    train_subset = Subset(dataset, train_indices)\n",
        "    val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "    # create dataloaders\n",
        "    train_dataloader = DataLoader(train_subset, batch_size=best_batch_size, shuffle=True, num_workers=num_workers)\n",
        "    val_dataloader = DataLoader(val_subset, batch_size=best_batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    # initialize the model for the current fold\n",
        "    model = CNN_v2(out_features=num_classes, hidden_units=best_hidden_units_combo, activation_functions=best_activation_functions, dropout_probs=best_dropout_probs_combo).to(device)\n",
        "\n",
        "    # in case we load another model's best weights\n",
        "    if load_weights:\n",
        "      print(\"Loading best weights ...\")\n",
        "      model.load_state_dict(best_weights) # must be of the same form/architecture, e.g. CNN_v2!\n",
        "\n",
        "    # set loss function and optimizer\n",
        "    optimizer = best_optimizer(params=model.parameters(), lr=best_learning_rate, weight_decay=best_decay_value)\n",
        "\n",
        "    # start training\n",
        "    start_time = time.time()\n",
        "    model_results = train(model=model,\n",
        "                          train_dataloader=train_dataloader,\n",
        "                          test_dataloader=val_dataloader,\n",
        "                          optimizer=optimizer,\n",
        "                          loss_fn=loss_fn,\n",
        "                          epochs=epochs,\n",
        "                          device=device)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # calculate training time and store results\n",
        "    model_results['training_time'] = end_time - start_time\n",
        "    model_results['trainable_params'] = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    model_results['model_weights'] = model.state_dict()\n",
        "\n",
        "    # save results\n",
        "    cross_valid_results[fold] = model_results\n",
        "\n",
        "    # print validation accuracy for the current fold\n",
        "    print(f\"Validation accuracy: {model_results['val_acc'][-1]:.4f}\\n\")\n",
        "\n",
        "  # compute final validation accuracy (mean of all final validation accuracies)\n",
        "  val_accuracies = [results['val_acc'][-1] for results in cross_valid_results.values()]\n",
        "  final_val_acc = sum(val_accuracies) / len(val_accuracies)\n",
        "  final_std = np.std(val_accuracies)\n",
        "  print(\"\\nCross-validation results:\")\n",
        "  print(f\"Final validation accuracy Mean: {final_val_acc:.4f}\") # want high mean val accuracy!\n",
        "  print(f\"Final validation accuracy Standard Deviation: {final_std:.4f}\") # want small std=>high confidence that model generalizes well!\n",
        "\n",
        "  return cross_valid_results\n",
        "\n",
        "\n",
        "# experiment 19\n",
        "def experiment_balanced_dataset(model, train_dataloader, val_dataloader, num_epochs, device, best_config, class_weights):\n",
        "  \"\"\" Experiment with a training on the initial, but balanced dataset. \"\"\"\n",
        "  print(\"Experimenting with Balanced Dataset\")\n",
        "\n",
        "  # unpack what's useful from best_config dictionary\n",
        "  best_decay_value = best_config['best_decay_value']\n",
        "  best_optimizer = best_config['best_optimizer']\n",
        "  best_learning_rate = best_config['best_learning_rate']\n",
        "\n",
        "  # set the loss function with class weights and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
        "  optimizer = best_optimizer(params=model.parameters(), lr=best_learning_rate, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training with early stopping\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_dataloader,\n",
        "                  test_dataloader=val_dataloader,\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device,\n",
        "                  early_stopping=True,  # enabled early stopping\n",
        "                  patience=5,  # epochs to wait\n",
        "                  delta=0.005)  # minimum improvement\n",
        "  end_time = time.time()\n",
        "\n",
        "  # calculate and print total training time\n",
        "  training_time = end_time - start_time\n",
        "  print(f\"\\nTotal training time: {training_time:.3f} seconds\")\n",
        "\n",
        "  # calculate and print total trainable parameters\n",
        "  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(\"Total trainable params:\", total_params)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "# experiment 20\n",
        "def experiment_data_augmentation(model, train_dir, val_dataloader, num_epochs, device, best_config, class_weights, num_workers, mean, std, is_Trivial=True):\n",
        "  \"\"\" Experiment with a training on a balanced dattaset with data augmentation. \"\"\"\n",
        "  print(\"Experimenting with Data Augmentation\")\n",
        "\n",
        "  # unpack the best_config dictionary\n",
        "  best_decay_value = best_config['best_decay_value']\n",
        "  best_optimizer = best_config['best_optimizer']\n",
        "  best_learning_rate = best_config['best_learning_rate']\n",
        "  best_batch_size = best_config['best_batch_size']\n",
        "\n",
        "  if is_Trivial:\n",
        "    # create augmented transform\n",
        "    augmented_transform = transforms.Compose([\n",
        "        transforms.TrivialAugmentWide(),  # automatically applies a random augmentation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "  else:\n",
        "    # another very simple augmented trasnform\n",
        "    augmented_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(), # random horizontal flip\n",
        "        #transforms.RandomVerticalFlip(), # random vertical flip\n",
        "        transforms.RandomRotation(degrees=10), # random rotation +/- 10\n",
        "        #transforms.RandomResizedCrop(size=(25, 149), scale=(0.8, 1.0)), # random cropping to (25, 129) with zoom 80%-100%\n",
        "        #transforms.RandomAffine(degrees=20, scale=(0.8, 1.2)),  # rotates ±20° and zoom 80%-120%\n",
        "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # random brightness, contrast, saturation, hue\n",
        "        transforms.ToTensor(),  # convert images to tensors\n",
        "        transforms.Normalize(mean=mean, std=std)\n",
        "        ])\n",
        "\n",
        "  # create new train, transformed dataset\n",
        "  train_aug_data = ImageFolder(root=train_dir, transform=augmented_transform)\n",
        "\n",
        "  # verify the new train dataset\n",
        "  # print(f\"Number of training samples: {len(train_aug_data)}\")\n",
        "  # print(f\"Class names: {train_aug_data.classes}\")\n",
        "\n",
        "  # create new train dataloader\n",
        "  train_aug_dataloader = DataLoader(dataset=train_aug_data, batch_size=best_batch_size, shuffle=True, num_workers=num_workers)\n",
        "\n",
        "  # set loss function and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss(weight=class_weights) if class_weights is not None else nn.CrossEntropyLoss()\n",
        "  optimizer = best_optimizer(params=model.parameters(), lr=best_learning_rate, weight_decay=best_decay_value)\n",
        "\n",
        "  # start training with early stopping\n",
        "  start_time = time.time()\n",
        "  results = train(model=model,\n",
        "                  train_dataloader=train_aug_dataloader,\n",
        "                  test_dataloader=val_dataloader, # validation dataloader is preloaded\n",
        "                  optimizer=optimizer,\n",
        "                  loss_fn=loss_fn,\n",
        "                  epochs=num_epochs,\n",
        "                  device=device,\n",
        "                  early_stopping=True, # enabled early stopping\n",
        "                  patience=5,\n",
        "                  delta=1e-5)\n",
        "  end_time = time.time()\n",
        "\n",
        "  # calculate and print total training time\n",
        "  training_time = end_time - start_time\n",
        "  print(f\"\\nTotal training time: {training_time:.3f} seconds\")\n",
        "\n",
        "  # calculate total trainable parameters\n",
        "  total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  print(f\"Total trainable params: {total_params}\")\n",
        "\n",
        "  return results\n",
        "\n",
        "print(\"Module experiments imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iSK8OXdGMQj",
        "outputId": "40c226d2-91e9-4bfa-ffbe-6045cbcca936"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/experiments.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils.py: contains various utility functions."
      ],
      "metadata": {
        "id": "0Xfy34_y3iHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/utils.py\n",
        "\"\"\" Contains various utility functions. \"\"\"\n",
        "\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from torchmetrics.classification import Precision, Recall, F1Score\n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "\n",
        "\n",
        "def set_seeds():\n",
        "  \"\"\" Set random seeds for reproducibility. \"\"\"\n",
        "  torch.manual_seed(42)\n",
        "  torch.cuda.manual_seed(42)\n",
        "\n",
        "\n",
        "def plot_images_per_class_big(all_M_images, all_L_images, all_R_images, label_dict):\n",
        "  \"\"\" Plots 5 random images per class for M, L, and R datasets, with labels. \"\"\"\n",
        "\n",
        "  plt.figure(figsize=(15, 20))\n",
        "  rows, columns = len(label_dict) * 3, 5  # 3 datasets, 5 samples per class\n",
        "\n",
        "  for i, (label_id, label_name) in enumerate(label_dict.items()):\n",
        "\n",
        "    # get all image paths for the current label in each dataset\n",
        "    image_paths_M = [path for path in all_M_images if int(path.split('/')[-1].split('.')[-3][-2:]) == label_id]\n",
        "    image_paths_L = [path for path in all_L_images if int(path.split('/')[-1].split('.')[-3][-2:]) == label_id]\n",
        "    image_paths_R = [path for path in all_R_images if int(path.split('/')[-1].split('.')[-3][-2:]) == label_id]\n",
        "\n",
        "    # select 5 random images for the current label in each dataset\n",
        "    if len(image_paths_M) >= 5 and len(image_paths_L) >= 5 and len(image_paths_R) >= 5:\n",
        "      random_images_M = random.sample(image_paths_M, 5)\n",
        "      random_images_L = random.sample(image_paths_L, 5)\n",
        "      random_images_R = random.sample(image_paths_R, 5)\n",
        "\n",
        "      # plot images for M dataset\n",
        "      for j, image_path in enumerate(random_images_M):\n",
        "        plt.subplot(rows, columns, i * 15 + j + 1)  # plot every 15 subplots for each class\n",
        "        image = plt.imread(image_path)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"M: {label_name}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "      # plot images for L dataset\n",
        "      for j, image_path in enumerate(random_images_L):\n",
        "        plt.subplot(rows, columns, i * 15 + j + 6)  # plot every 15 subplots for each class and second row (j+5)\n",
        "        image = plt.imread(image_path)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"L: {label_name}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "      # plot images for R dataset\n",
        "      for j, image_path in enumerate(random_images_R):\n",
        "        plt.subplot(rows, columns, i * 15 + j + 11)  # plot every 15 subplots for each class and third row (j+5 from the prvious state)\n",
        "        image = plt.imread(image_path)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"R: {label_name}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_images_per_class_small(all_small_images, small_label_list):\n",
        "  \"\"\" Plots 5 random images per class for small dataset, with labels. \"\"\"\n",
        "\n",
        "  plt.figure(figsize=(15, 20))\n",
        "  rows, columns = len(small_label_list), 5  # 1 dataset, 5 samples per class\n",
        "\n",
        "  for i, label_name in enumerate(small_label_list): # loop over the list and the index i\n",
        "\n",
        "    # get all image paths for the current label in the small dataset\n",
        "    image_paths_small = [path for path in all_small_images if os.path.basename(path).split('.')[0][:-3] == label_name] # os.path.basename: extract only the name from the path (it ccame after error!)\n",
        "\n",
        "    # select 5 random images for the current label\n",
        "    if len(image_paths_small) >= 5:\n",
        "      random_images_small = random.sample(image_paths_small, 5)\n",
        "\n",
        "      for j, image_path in enumerate(random_images_small):\n",
        "        plt.subplot(rows, columns, i * columns + j + 1)\n",
        "        image = plt.imread(image_path)\n",
        "        plt.imshow(image)\n",
        "        plt.title(f\"{label_name}\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_curves(results):\n",
        "  \"\"\" Plots training curves of a results dictionary. \"\"\"\n",
        "\n",
        "  # get all values from the dictionary\n",
        "  loss = results['train_loss']\n",
        "  val_loss = results['val_loss']\n",
        "  acc = results['train_acc']\n",
        "  val_acc = results['val_acc']\n",
        "\n",
        "  # also figure out in how many epochs training occured\n",
        "  epochs = range(len(results['train_loss']))\n",
        "\n",
        "  # setup a plot\n",
        "  plt.figure(figsize=(15, 5))\n",
        "\n",
        "  # plot losses\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs, loss, label='train_loss')\n",
        "  plt.plot(epochs, val_loss, label='val_loss')\n",
        "  plt.title('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  # plot accuracies\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs, acc, label='train_acc')\n",
        "  plt.plot(epochs, val_acc, label='val_acc')\n",
        "  plt.title('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_hidden_layer_results(hidden_size_df):\n",
        "  \"\"\" Plots a bar chart for hidden layer sizes vs. validation accuracy. \"\"\"\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  # convert tuple values to string for plotting\n",
        "  hidden_layer_labels = []\n",
        "  for h in hidden_size_df['Hidden Layer Sizes']:\n",
        "    h = str(h)\n",
        "    hidden_layer_labels.append(h)\n",
        "  validation_accuracies = hidden_size_df['Validation Accuracy']\n",
        "\n",
        "  # create a horizaontal bar plot\n",
        "  plt.barh(hidden_layer_labels, validation_accuracies)\n",
        "\n",
        "  # labels and title\n",
        "  plt.xlabel(\"Validation Accuracy\")\n",
        "  plt.ylabel(\"Hidden Layer Sizes\")\n",
        "  plt.title(\"Validation Accuracy for Different Hidden Layer Sizes\")\n",
        "  plt.gca().invert_yaxis()  # Invert y-axis to have the best at the top\n",
        "\n",
        "  # show values on bars\n",
        "  for index, value in enumerate(validation_accuracies):\n",
        "    plt.text(value, index, f\"{value:.4f}\")\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_optimizers_results(results_df):\n",
        "  \"\"\" Plots the results of the experiment_optimizers_and_learning_rates function. \"\"\"\n",
        "\n",
        "  # ensure results_df is sorted\n",
        "  results_df = results_df.sort_values(by='Validation Accuracy', ascending=False)\n",
        "\n",
        "  # extract optimizer, learning rate and validation accuracy\n",
        "  optimizer_labels = []\n",
        "  for index, row in results_df.iterrows():\n",
        "    optimizer_labels.append(f\"{row['Optimizer']} - {row['Best Learning Rate']}\")\n",
        "  validation_accuracies = results_df['Validation Accuracy'].tolist() # need to convert tolist in order to plot it below\n",
        "\n",
        "  # reverse the order so the best optimizer appears at the top\n",
        "  optimizer_labels.reverse()\n",
        "  validation_accuracies.reverse()\n",
        "\n",
        "  # create a horizontal barplot\n",
        "  plt.figure(figsize=(12,  6))\n",
        "  plt.barh(optimizer_labels, validation_accuracies)\n",
        "\n",
        "  # labels and title\n",
        "  plt.xlabel('Validation Accuracy')\n",
        "  plt.ylabel('Optimizer and Learning Rate')\n",
        "  plt.title('Validation Accuracy by Optimizer and Learning Rate')\n",
        "\n",
        "  # show values on bars\n",
        "  for index, value in enumerate(validation_accuracies):\n",
        "    plt.text(value, index, f'{value:.4f}')\n",
        "\n",
        "  # show the plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_activation_function_results(results_df):\n",
        "  \"\"\" Plots a bar chart for activation functions combinataions vs. validation accuracy. \"\"\"\n",
        "\n",
        "  # ensure the dataframe iis sorted\n",
        "  results_df = results_df.sort_values(by='Validation Accuracy', ascending=False)\n",
        "\n",
        "  # create labels by combining activation function 1 and activation function 2\n",
        "  labels = [f\"{row['Activation Function 1']} + {row['Activation Function 2']}\" for _, row in results_df.iterrows()]\n",
        "  validation_accuracies = results_df['Validation Accuracy']\n",
        "\n",
        "  # create a horizontal bar chart\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.barh(labels, validation_accuracies)\n",
        "\n",
        "  # labels and title\n",
        "  plt.xlabel('Validation Accuracy')\n",
        "  plt.ylabel('Activation Functions Combination')\n",
        "  plt.title('Activation Functions Combinations vs. Validation Accuracy')\n",
        "  plt.gca().invert_yaxis() # best at the top (another way than above)\n",
        "\n",
        "  # show values on bars\n",
        "  for index, value in enumerate(validation_accuracies):\n",
        "    plt.text(value, index, f'{value:.4f}')\n",
        "\n",
        "  # show the plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_cv_results(cv_results_df):\n",
        "  \"\"\" Plots a bar chart for cross-validation folds vs. validation accuracy. \"\"\"\n",
        "\n",
        "  # ensure the dataframe is sorted\n",
        "  cv_results_df = cv_results_df.sort_values(by='Validation Accuracy', ascending=True)\n",
        "\n",
        "  # extract fold numbers and their validation accuracies\n",
        "  folds = [f\"Fold {fold}\" for fold in cv_results_df['Fold']] # ensures correct indexing!\n",
        "  val_accs = cv_results_df['Validation Accuracy']\n",
        "\n",
        "  # create a horizontal bar chart\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.barh(folds, val_accs)\n",
        "\n",
        "  # labels and title\n",
        "  plt.xlabel('Validation Accuracy')\n",
        "  plt.ylabel('Fold')\n",
        "  plt.title('Cross-Validation Results')\n",
        "\n",
        "  # show accuracy values on bars\n",
        "  for index, value in enumerate(val_accs):\n",
        "    plt.text(value, index, f'{value:.4f}')\n",
        "\n",
        "  # show the plot\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def print_best_epoch_and_val_acc(results):\n",
        "  \"\"\" Prints the best epoch and its corresponding validation accuracy. \"\"\"\n",
        "\n",
        "  best_epoch = results['best_epoch']\n",
        "  print(f\"Best epoch: {best_epoch}\")\n",
        "  best_val_acc = results['val_acc'][best_epoch - 1] # (val_acc is zero based)\n",
        "  print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "  return best_epoch, best_val_acc\n",
        "\n",
        "\n",
        "def print_and_plot_models_accuracies(df):\n",
        "  \"\"\" Prints and plots the dataframe of models accuracies. \"\"\"\n",
        "\n",
        "  #print(df)\n",
        "  for index, row in df.iterrows():\n",
        "    print(f\"Model {row['Model']}: {row['Validation Accuracy']:.8f}\")\n",
        "\n",
        "  # plot the results\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  plt.barh(df['Model'], df['Validation Accuracy'])\n",
        "  plt.ylabel('Model')\n",
        "  plt.xlabel('Validation Accuracy')\n",
        "  plt.title('Validation Accuracy Comparison')\n",
        "\n",
        "  # display the values on bars\n",
        "  for index, value in enumerate(df['Validation Accuracy']):\n",
        "    plt.text(value, index, f'{value * 100:.2f}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def store_best_model(df, dataset_name, best_models_dict):\n",
        "  \"\"\" Finds the best performing model in respect to the accuracy of df and stores it in best_models_dict. \"\"\"\n",
        "\n",
        "  # identify the row wit the highest test avccuracy and retrieve it along with its corresponding model name\n",
        "  best_model_index = df['Validation Accuracy'].idxmax()\n",
        "  best_model_name = df.loc[best_model_index, 'Model']\n",
        "  best_model_test_acc = df.loc[best_model_index, 'Validation Accuracy']\n",
        "\n",
        "  # update the dictionary with best models info\n",
        "  best_models_dict[dataset_name] = {'Model': best_model_name, 'Validation Accuracy': best_model_test_acc}\n",
        "\n",
        "  return best_models_dict\n",
        "\n",
        "\n",
        "def predict_from_dataloader(model, test_dataloader, device, index=0):\n",
        "  \"\"\" Extract a single sample from the test dataloader and predict its label. \"\"\"\n",
        "\n",
        "  # retrieve one batch of images and labels\n",
        "  img_batch, label_batch = next(iter(test_dataloader))\n",
        "\n",
        "  # get a single image from the batch and unsqueeze it to meet the shape of the model\n",
        "  img_single, label_single = img_batch[index].unsqueeze(0).to(device), label_batch[index]\n",
        "\n",
        "  # perform a forward pass on a single image = predict the label of the single image\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    pred_logits = model(img_single.to(device)) # remember our model is on device!\n",
        "\n",
        "  # print out what's happening, converting logits->probs->labels\n",
        "  print(f\"Image shape: {img_single.shape} -> [batch_size, color_channels, height, width]\\n\")\n",
        "  print(f\"Output logits:\\n{pred_logits}\\n\")\n",
        "  print(f\"Output prediction probabilities:\\n{torch.softmax(pred_logits, dim=1)}\\n\")\n",
        "  print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred_logits, dim=1), dim=1)}\\n\")\n",
        "  print(f\"Actual label:\\n{label_single}\")\n",
        "\n",
        "\n",
        "def save_model(model, target_dir, model_name):\n",
        "  \"\"\" Saves a PyTorch model to a target directory. \"\"\"\n",
        "\n",
        "  # create target directory, if not exists\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  # create model save path\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path / model_name\n",
        "\n",
        "  # save the model state_dict()\n",
        "  torch.save(obj=model.state_dict(), f=model_save_path)\n",
        "  print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "\n",
        "def compute_multi_generalization_gaps(results_dict):\n",
        "  \"\"\" Computes the generalization gap (train acc - val acc) given many model's results (experiments) dictionary. \"\"\"\n",
        "  differences = {}\n",
        "  for decay, results in results_dict.items():\n",
        "    diff = results[\"train_acc\"][-1] - results[\"val_acc\"][-1]\n",
        "    differences[decay] = diff\n",
        "    print(f\"Difference between train and val acc for {decay}: {diff:.4f}\")\n",
        "\n",
        "  # Find the best/smallest gap\n",
        "  min_diff = min(differences.values())\n",
        "  best_value = [decay for decay, diff in differences.items() if diff == min_diff][0]\n",
        "  print(f\"\\nThe best (smallest) generalization gap is for {best_value} and is {min_diff:.4f}\")\n",
        "\n",
        "  return best_value, min_diff\n",
        "\n",
        "\n",
        "def compute_generalization_gap(results):\n",
        "  \"\"\" Computes the generalization gap (train acc - val acc) given a model's training results dictionary. \"\"\"\n",
        "\n",
        "  train_acc_last = results[\"train_acc\"][-1]\n",
        "  val_acc_last = results[\"val_acc\"][-1]\n",
        "  diff = train_acc_last - val_acc_last\n",
        "  print(f\"Generalization gap (train acc - val acc): {diff:.4f}\")\n",
        "\n",
        "  return diff\n",
        "\n",
        "\n",
        "def compute_avg_cv_results(cross_validation_results):\n",
        "  \"\"\" Computes the averaged learning curves (train_loss, val_loss, train_acc, val_acc) across all folds of all epochs of cross validation results. \"\"\"\n",
        "\n",
        "  # find the number of folds and epochs\n",
        "  num_folds = len(cross_validation_results)\n",
        "  num_epochs = len(cross_validation_results[0]['train_loss']) # take for example the length of one metric\n",
        "\n",
        "  # initialize arrays to hold the sum over epochs\n",
        "  sum_train_loss = np.zeros(num_epochs)\n",
        "  sum_val_loss = np.zeros(num_epochs)\n",
        "  sum_train_acc = np.zeros(num_epochs)\n",
        "  sum_val_acc = np.zeros(num_epochs)\n",
        "\n",
        "  # loop over each fold and accumulate the metrics\n",
        "  for fold, results in cross_validation_results.items():\n",
        "    sum_train_loss += np.array(results['train_loss'])\n",
        "    sum_val_loss +=  np.array(results['val_loss'])\n",
        "    sum_train_acc +=  np.array(results['train_acc'])\n",
        "    sum_val_acc +=  np.array(results['val_acc'])\n",
        "\n",
        "  # compute the average for each metric across all folds\n",
        "  avg_train_loss = sum_train_loss / num_folds\n",
        "  avg_val_loss = sum_val_loss / num_folds\n",
        "  avg_train_acc = sum_train_acc / num_folds\n",
        "  avg_val_acc = sum_val_acc / num_folds\n",
        "\n",
        "  # create a dictionary with the averaged results\n",
        "  cv_averaged_results = {\n",
        "      'train_loss': avg_train_loss,\n",
        "      'val_loss': avg_val_loss,\n",
        "      'train_acc': avg_train_acc,\n",
        "      'val_acc': avg_val_acc\n",
        "  }\n",
        "\n",
        "  return cv_averaged_results\n",
        "\n",
        "\n",
        "def evaluate_model_performance_using_torchmetrics(model_y_pred, model_y_true, test_data_classes):\n",
        "  \"\"\" Evaluates model performance by computing a confusion matrix, precision, recall, and F1-score. \"\"\"\n",
        "\n",
        "  # setup confusion matrix instance and compute confusion matrix\n",
        "  confmat = ConfusionMatrix(num_classes=len(test_data_classes), task='multiclass')\n",
        "  confmat_tensor = confmat(model_y_pred.clone().detach(), model_y_true.clone().detach())\n",
        "\n",
        "  # display the confusion matrix\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confmat_tensor)\n",
        "  print(\" \")\n",
        "\n",
        "  # plot the confusion matrix\n",
        "  fig, ax = plot_confusion_matrix(conf_mat=confmat_tensor.numpy(), class_names=test_data_classes, figsize=(10, 6))\n",
        "  plt.show()\n",
        "\n",
        "  # compute classification metrics\n",
        "  precision = Precision(average='macro', num_classes=len(test_data_classes), task='multiclass')\n",
        "  recall = Recall(average='macro', num_classes=len(test_data_classes), task='multiclass')\n",
        "  f1 = F1Score(average='macro', num_classes=len(test_data_classes), task='multiclass')\n",
        "\n",
        "  prec = precision(model_y_pred, model_y_true)\n",
        "  rec = recall(model_y_pred, model_y_true)\n",
        "  f1_score = f1(model_y_pred, model_y_true)\n",
        "\n",
        "  # print the calculated scores\n",
        "  print(f\"\\nPrecision: {prec:.4f}\\nRecall: {rec:.4f}\\nF1-Score: {f1_score:.4f}\")\n",
        "\n",
        "  # print classification report\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_true=model_y_true.cpu().numpy(), y_pred=model_y_pred.cpu().numpy()))\n",
        "\n",
        "  # return results as tensors in a dictionary\n",
        "  return {\n",
        "      \"precision\": prec, # .item() for just scalars\n",
        "      \"recall\": rec,\n",
        "      \"f1_score\": f1_score\n",
        "      }\n",
        "\n",
        "\n",
        "def get_sorted_cv_results(cv_results):\n",
        "  \"\"\" Extracts validation accuracies and model weights from the cross validation results dictionary and returns a sorted dataframe in desc order of validation accuracy. \"\"\"\n",
        "  results_list = []\n",
        "  for fold, results in cv_results.items():\n",
        "    val_acc = results['val_acc'][-1]\n",
        "    model_weights = results['model_weights']\n",
        "    results_list.append({'Fold': fold + 1, 'Validation Accuracy': val_acc, 'Model Weights': model_weights})\n",
        "  results_df = pd.DataFrame(results_list)\n",
        "  results_df = results_df.sort_values(by='Validation Accuracy', ascending=False)\n",
        "  print(results_df) # print for vizualization\n",
        "  return results_df\n",
        "\n",
        "\n",
        "print(\"Module utils imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHShiy898ayk",
        "outputId": "25e47eab-8727-4f64-fd77-d58481a96373"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save and download the parent src directory in drive."
      ],
      "metadata": {
        "id": "mYRlgHEn4WxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zip the dir with all python files\n",
        "!zip -r src.zip src\n",
        "\n",
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# define destination path\n",
        "destination_path = '/content/drive/MyDrive/src.zip'\n",
        "\n",
        "# move zip to drive\n",
        "!mv src.zip '{destination_path}'\n",
        "if os.path.exists(destination_path):\n",
        "  print(f\"Saved to Google Drive at {destination_path}\")"
      ],
      "metadata": {
        "id": "7kCRmbaC8avs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3424a53d-3238-4ca2-876b-9ca208ab9659"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: src/ (stored 0%)\n",
            "  adding: src/models.py (deflated 76%)\n",
            "  adding: src/data_utils.py (deflated 74%)\n",
            "  adding: src/engine.py (deflated 73%)\n",
            "  adding: src/experiments.py (deflated 89%)\n",
            "  adding: src/config.py (deflated 42%)\n",
            "  adding: src/utils.py (deflated 73%)\n",
            "Mounted at /content/drive\n",
            "Saved to Google Drive at /content/drive/MyDrive/src.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vR1s9JtpQahH"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}